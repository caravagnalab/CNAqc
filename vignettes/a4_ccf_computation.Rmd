---
title: "4. Computation of Cancer Cell Fractions"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{4. Computation of Cancer Cell Fractions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

options(crayon.enabled=F)
```


# Phasing mutations

```{r setup}
library(CNAqc)
```


CNAqc can compute Cancer Cell Fractions (CCFs) using function `compute_CCF`, which can be used to compare CCF estimates obtained with other tools (e.g., DPClust, Ccube, etc.). The main advantage of CNAqc is reporting uncertainty/ unreliability of a CCF estimate.

To compute CCFs CNAqc computes the _mutation multiplicity_ for every mutation (we call this task _phasing_): this is defined as the number of copies of a mutation in a certain copy number segment  This is a difficult task that harmonize the data accounting for tumour purity, and CNAs;  phasing errors propagate in CCF estimates and downstream analyses.

CNAqc restricts CCF computation to simple clonal CNA segments, and offers two algorithms to phase mutations. When CCFs are computed the S3 `print` method reports the relevant information. 

```{r, fig.width=6, fig.height=4, message=F, warning=F}
data("example_PCAWG", package = 'CNAqc')

print(example_PCAWG)
```

## Entropy-based 

The entropy-based approach to compute CCFs is the more advanced  available in CNAqc, and the only one that provides CCF uncertainty estimation. 

The idea is to use the entropy of a mixture model for the data distribution in order to detect low-uncertainty CCF estimates. Two Binomial distributions are used for the mutations happening before, and after aneuploidy. The density of these Binomial distributions are:

* peaked at $v_1$/$v_2$  for $m = 1$ (after CNA) and $m = 2$  (before CNA); $v_i$ are peaks determined as in peak-analysis procedures'.' 
* computed on the domain $[0, 1]$, after translating read counts into VAFs. The number of trials of the Binomial process is set to the median coverage $n$ of the observed mutations that map to the segments under investigation.

The assumptions of CNAqc are that: 

- coverage overdispersion  is small to justify a Binomial instead of a Beta-Binomial;
- trials are well-represented by the median of the observed coverage.

**Mixture construction.** CNAqc computes two Binomial densities $\text{Bin}_1$ and $\text{Bin}_2$ over the set of values $[0; n]$ with $n$ number of trials and success probability $v_i$, i.e.
\[
\text{Bin}_i = p(s \mid n; v_i)
\]
as the probability of finding $s$ reads with the mutant allele at out of $n$ (median depth), assuming the mutation expected frequency is $v_i$ (recall that $v_i$ includes the effect of tumour purity).

To finalize the densities of this mixture CNAqc determines the proportions of the mutations per component; this is done by counting how many mutations fall below the area of each Binomial density, as defined by the 1% and 99% quantile of the components distribution. Two ranges $I_1 = [v_1^1; v_1^{99}]$ and $I_2 = [v_2^1; v_2^{99}]$ are computed so that:

* $n_1$ are the number of mutations with VAF in $I_1$; 
* $n_2$ are the number of mutations with VAF in $I_2$; 

Notice that the intervals might overlap (i.e., $v_1^{99} > v_2^{1}$). Also, note that mutations with VAF below the smallest interval point ($I_1 < v_1^1$) are subclonal and not counted in $n_1$, i.e., the mutations accrued after the CNA.  The counts  are normalized to create a complete two components Binomial density mixture
\[
M = \mu_1 * \text{Bin}_1 + (1 - \mu_1) * \text{Bin}_2
\]
where $\mu_1 = n_1/(n_1+n_2)$ and $\mu_2 = 1 - \mu_1$ are the normalized  proportions. 

**Entropy-based assignments.** 

Assigning mutation multiplicities is particularly difficult at the crossing of the two Binomial densities. Mutations "in between" the densities $\text{Bin}_1$ and $\text{Bin}_1$ could have  multiplicity $m=1$ or $m=2$, and might be mis-assigned. This error, if not corrected for, determines fake peaks in the CCF distribution which might be either confounded for miscalled copy-number segments or false [subclonal expansions](https://caravagnalab.github.io/mobster/). 

CNAqc uses an entropy-based heuristic to assess whether it is possible to confidently assign $m$ to each VAF value. Mutations with VAF below $v_1$ are in one copy ($m=1$), those above $v_2$ in two copies ($m=2$). CNAqc uses the entropy $H(x)$ 
\[
H(x) = - p_x \log(p_x)
\]
for VAF value $x$ as defined from the mixture's latent variables to determine mutation multiplicities, where $p_x$ is the value of the mixture density at $x$.

Peaks in $H(x)$  identify VAF ranges where one of the two densities is dominated by the other, and therefore the value of $m$ can be estimated confidently. This consists in finding two peaks $\rho_1$ and $\rho_2$ and setting $m=1$ for $x < \rho_1$, $m=2$ for $x > \rho_2$, and `NA` for $\rho_1 \leq x \leq \rho_2$. Values for $\rho_i$ are found using [peakPick](https://cran.rstudio.com/web/packages/peakPick/index.html) as for QC metrics.

CNAqc computes a QC score for CCFs; a karyotype is considered `PASS` is less than `p%` (default `10%`) mutations are not assignable (`CCF = NA`). The computation results can be plot. 

```{r, fig.width=6, fig.height=4, message=F, warning=F}
# We do not assemble the fits
plot_CCF(example_PCAWG, assembly_plot = F)
```
The plot shows the quantities described above; from left to right we have

* the CCF histogram, coloured by mutation multiplicity $m$;
* the histogram of the input VAF, coloured by mutation multiplicity $m$. In this plot the range between the red dashed lines represent the areas where it is not possible to compute reliably CCF values;
* the entropy profile $H(x)$. Here notice that the points we seek to identify are those where $H(x)$ peaks higher (indeed matched). The width of the entropy peak that determines un-assignable mutations is proportional to the variance of the counts, and therefore the sequencing coverage;
* the proportion of mutations assigned for each value of $m$, and those unassigned.

The rightmost CCF histogram plot is also surrounded by a green or red square to reflect QC status of `PASS` (green) or `FAIL` (red).


## Hard-cut based 

A method is available to compute CCFs regardless of the entropy $H(x)$. When we have computed the
2-class Binomial mixture, CNAqc uses the means of the parameters of the Binomial distributions to determine a hard split of the data.

Define $p_1$ and $p_2$ the two means; the midpoint would be centered at $\hat{p} = p1 + (p_2-p_1)/2$, we shift it proportionally to size ($n_i$) of the mixture components 
\[
\hat{p} = p_1 + (p_2 - p_1) * \mu_1
\]
where $\mu_1 = n_1/(n_1+n_2)$ is the normalized  proportions. 

We set $m=1$ for $x \leq \hat{p}$, and $m=2$ otherwise. Since there are no `NA` assignments, the computation is always scored `PASS` for  QC purposes; for this reason this computation is more "rough" than the one based on entropy.

# Example computation

We work with the template dataset. 

```{r echo=TRUE, results=TRUE, message=FALSE}
# Dataset available with the package 
data('example_dataset_CNAqc', package = 'CNAqc')

x = CNAqc::init(
  mutations = example_dataset_CNAqc$mutations, 
  cna = example_dataset_CNAqc$cna,
  purity = example_dataset_CNAqc$purity,
  ref = 'hg19')

print(x)
```

We run function `compute_CCF`. Without parameters (or with `method = 'ENTROPY'`), we use the entropy method. We note that in general one should check peaks-based QC and use only karyotypes that pass that analysis; for this sample, all of them pass.
```{r,fig.width=9, fig.height=5, warning=F, message=T}
x = compute_CCF(x)

# Print new object, it informs us of the available computation
print(x)
```

A tibble of CCF values (getter function `CCF`)  reports the computed values  in column `"CCF"`, and in column `"mutation multiplicity"`. 

```{r,fig.width=11, fig.height=7}
# Tibble
CCF(x) %>% select(VAF, mutation_multiplicity, CCF)
```

Fit plot as above.
```{r,fig.width=11, fig.height=4,warning=F}
# We just omit empty plots with empty_plot = FALSE
plot_CCF(x, empty_plot = FALSE)
```

The CCF histogram can be computed with `plot_data_histogram`, specifying `CCF` as parameter.
```{r,fig.width=4, fig.height=4,warning=F}
plot_data_histogram(x, which = 'CCF')
```

We can compare the two CCF methods on the same data.
```{r,fig.width=9, fig.height=5, warning=F, message=T}
x_r = compute_CCF(x, method = 'ROUGH')

print(x_r)
```

The data can be accessed in the usual way; the layout of the CCF plots are conceptually the same, but use different colors. The cutoff is annotated in the panel with the mutation multiplicity $m$ assigned to each observed VAF value.
```{r,fig.width=11, fig.height=4,warning=F}
plot_CCF(x_r, empty_plot = FALSE)
```

Notice that for karyotype `2:0` the difference among the mutation multiplicity histograms (second panel) obtained from the two methods is minimal. The difference is also small for triploid mutations (`'2:1'`). The results for tetraploid mutations is instead quite different, and that's because there are not many mutations happening after genome doubling (leftmost peak, $m=1$), and therefore the weighted split of the rough method tends to assign most mutations to the rightmost peak ($m=2$).

A visual plot is obtained using the internal function `CNAqc:::plot_mutation_multiplicity_rough`.
```{r,fig.width=9, fig.height=6,warning=F}
ggpubr::ggarrange(
  CNAqc:::plot_mutation_multiplicity_entropy(x, '2:2'),
  CNAqc:::plot_mutation_multiplicity_rough(x_r, '2:2'),
  nrow = 2
)
```


We can compare the two computed CCF distributions, and observe that the difference in the estimation from karyotype `'2:2'` creates a small bump at about `CCF=0.5`.
```{r,fig.width=8, fig.height=4,warning=F}
ggpubr::ggarrange(
  plot_data_histogram(x, which = 'CCF'),
  plot_data_histogram(x_r, which = 'CCF'),
  nrow = 1
)
```

## Decreasing coverage

Because these are high-quality data, the results between methods are quite similar. This is however not always true.

The uncertainty from the entropy method stems from the variance of the mixture components. Because these are Binomial distributions, the variance depends on the number of trials, so the sequencing coverage.

If we take the data of `x` and just scale by a constant the read counts, we preserve the same VAF distribution but increase the variance because we decrease coverage. Let's see this in practice retaining `70%` of the actual read counts.

```{r,fig.width=11, fig.height=10, warning=F, message=T}
x_s = init(
  mutations = x$mutations %>% mutate(DP = DP * .6, NV = NV * .6),
  cna = x$cna,
  purity = x$purity)

# Recompute CCF values
y_e = compute_CCF(x_s, method = 'ENTROPY', karyotypes = '2:2')
y_r = compute_CCF(x_s, method = 'ROUGH', karyotypes = '2:2')

CCF(y_e) %>% filter(!is.na(CCF))
CCF(y_r) %>% filter(!is.na(CCF))
```
Observe that there are about 400 less mutations with CCF values when we run the entropy method.

```{r,fig.width=9, fig.height=6,warning=F}
ggpubr::ggarrange(
  CNAqc:::plot_mutation_multiplicity_entropy(y_e, '2:2'),
  CNAqc:::plot_mutation_multiplicity_rough(y_r, '2:2'),
  nrow = 2
)
```

```{r,fig.width=8, fig.height=4,warning=F,message=FALSE}
ggpubr::ggarrange(
  plot_data_histogram(y_e, which = 'CCF', karyotypes = '2:2'),
  plot_data_histogram(y_r, which = 'CCF', karyotypes = '2:2'),
  nrow = 1
)
```
